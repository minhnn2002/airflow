from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime
import os

def download_from_s3(local_path, key, bucket_name):
    hook = S3Hook('aws_s3')
    hook.download_file(key=key, bucket_name=bucket_name, local_path=local_path, preserve_file_name=True, use_autogenerated_subdir=False)

with DAG(
    dag_id="restore_starrocks_fe",
    start_date=datetime(2025, 8, 11),
    schedule_interval=None,
    catchup=False,
) as dag:

    download = PythonOperator(
        task_id="download_from_s3",
        python_callable=download_from_s3,
        op_kwargs={
            'local_path': '/tmp',
            'key': 'fe/fe.tar.gz',
            'bucket_name': 'starrockk'
        }
    )

    copy_to_starrocks = BashOperator(
        task_id="copy_to_starrocks",
        bash_command=(
            'sshpass -p "password" scp -o StrictHostKeyChecking=no -P 2222 /tmp/fe.tar.gz root@starrocks-fe:/tmp/fe.tar.gz'
        )
    )

    extract_on_starrocks = BashOperator(
        task_id="extract_on_starrocks",
        bash_command=(
            'sshpass -p "password" ssh -o StrictHostKeyChecking=no -p 2222 root@starrocks-fe '
            '"tar -xzvf /tmp/fe.tar.gz -C /opt/starrocks/fe"'
        )
    )

  
    download >> copy_to_starrocks >> extract_on_starrocks
