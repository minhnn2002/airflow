# StarRocks Backup & Restore Guide

## Table of Contents
1. [Overview](#1-overview)
2. [MinIO Backup & Restore](#2-minio-backup--restore)
3. [FE Backup & Restore](#3-fe-backup--restore)

---

## 1. Overview
This document explains how to back up and restore **StarRocks** in a **shared-data architecture** setup.  
In this mode:
- **FE (Frontend)** stores **metadata** locally.
- **CN (Compute Node)** do the computational duty.
- **Distributed Storage (MinIO, S3,...)** where all table data is stored.

To fully preserve data, you must back up:
1. **MinIO data** (actual table files)
2. **FE metadata** (database definitions, schema, permissions, etc.)

We will use Airflow to orchestrate the process.

---

## 2. MinIO Backup & Restore

### 2.1 Backup MinIO
MinIO stores all StarRocks table data, organized by **cluster ID**, **database ID**, **table ID**, and **partition ID**.

We will use the [`mc mirror`](https://min.io/docs/minio/linux/reference/minio-mc/mc-mirror.html) command to synchronize data from MinIO to another storage location (S3, local, or another MinIO instance). Therefore we need to install mc in Airflow container.

#### Example: Backup to AWS S3
```bash
# Add aliases
mc alias set myminio http://minio:9000 MINIO_USER MINIO_PASSWORD
mc alias set mys3 https://s3.amazonaws.com AWS_ACCESS_KEY AWS_SECRET_KEY

# Mirror all StarRocks data to S3 bucket
mc mirror --overwrite myminio/starrocks mys3/starrocks-backup
```

### 2.2 Restore MinIO
Restoring MinIO means copying the backup data back to the original MinIO bucket with the same folder structure and cluster ID.  
If the cluster ID does not match, StarRocks will not recognize the restored data.

#### Example: Restore from AWS S3
```bash
mc alias set myminio http://minio:9000 MINIO_USER MINIO_PASSWORD
mc alias set mys3 https://s3.amazonaws.com AWS_ACCESS_KEY AWS_SECRET_KEY

# Mirror backup data back to MinIO
mc mirror --overwrite mys3/starrocks-backup myminio/starrocks
```

### 2.3 Using Airflow
Airflow can automate MinIO backup and restore using the `BashOperator` or Python hooks.

#### Airflow Task to Backup MinIO to AWS S3
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id='backup_be',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False,
) as dag:

    backup_be = BashOperator(
        task_id='backup_minio_to_s3',
        bash_command="""
        mc alias set myminio http://minio:9000 MINIO_USER MINIO_PASSWORD
        mc alias set mys3 https://s3.amazonaws.com AWS_ACCESS_KEY AWS_SECRET_KEY
        mc mirror --overwrite mys3/starrocks-backup/ myminio/starrocks
        """
    )

    backup_be
```

#### Airflow Task to Restore MinIO from AWS S3
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    dag_id='restore_be',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False,
) as dag:

    restore_be = BashOperator(
        task_id='restore_be',
        bash_command="""
        mc alias set myminio http://minio:9000 MINIO_USER MINIO_PASSWORD
        mc alias set mys3 https://s3.amazonaws.com AWS_ACCESS_KEY AWS_SECRET_KEY
        mc mirror --overwrite myminio/starrocks mys3/starrocks-backup/
        """
    )

    restore_be
```
---
## 3. FE Backup & Restore
FE metadata is stored in the FE containerâ€™s /opt/starrocks/fe directory.
Backing up the FE ensures that database definitions, schema information, and user permissions are preserved.

Since Airflow and StarRocks-FE run in separate containers, we need to SSH into the FE container from Airflow to perform the backup/restore.

### 3.1 FE Backup
The FE Backup process consists of **3 steps**:

1. **Archive FE metadata**  
   - Connect to the FE container/server via SSH.  
   - Compress the `/opt/starrocks/fe` directory into a `.tar.gz` file.  
  ```bash
'sshpass -p "password" ssh -o StrictHostKeyChecking=no '
'-p 2222 root@starrocks-fe '
'"tar -czvf /tmp/fe.tar.gz -C /opt/starrocks/fe ."'
```
2. **Copy archive file from FE container to Airflow container** 
  ```bash
'sshpass -p "password" scp -P 2222 root@starrocks-fe:/tmp/fe.tar.gz /tmp/fe.tar.gz'
```

3. **Upload the archive file to S3** 
  ```python
def upload_to_s3(filename, key, bucket_name):
    hook = S3Hook('aws_s3')
    hook.load_file(filename=filename, key=key, bucket_name=bucket_name)
```

### 3.2 FE Restore
The FE Backup process consists of **3 steps**:
1. **Download the file from S3**  
```python
def download_from_s3(local_path, key, bucket_name):
    hook = S3Hook('aws_s3')
    hook.download_file(key=key, bucket_name=bucket_name, local_path=local_path, preserve_file_name=True, use_autogenerated_subdir=False)
```
2. **Copy archive file from Airflow container to FE container** 
  ```bash
'sshpass -p "password" scp -o StrictHostKeyChecking=no -P 2222 /tmp/fe.tar.gz root@starrocks-fe:/tmp/fe.tar.gz'
```

3. **Extract the Backup file** 
  ```bash
'sshpass -p "password" ssh -o StrictHostKeyChecking=no -p 2222 root@starrocks-fe '
'"tar -xzvf /tmp/fe.tar.gz -C /opt/starrocks/fe"'
```

### 3.3 Using Airflow
Airflow can automate MinIO backup and restore using the `BashOperator` or Python hooks.

#### Airflow Task to Backup MinIO to AWS S3
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime

def upload_to_s3(filename, key, bucket_name):
    hook = S3Hook('aws_s3')
    hook.load_file(filename=filename, key=key, bucket_name=bucket_name)

with DAG(
    dag_id="backup_starrocks_fe",
    start_date=datetime(2025, 8, 11),
    schedule_interval="@daily",
    catchup=False,
) as dag:

    backup_and_copy = BashOperator(
        task_id="backup_and_copy",
        bash_command=(
            'sshpass -p "password" ssh -o StrictHostKeyChecking=no '
            '-p 2222 root@starrocks-fe '
            '"tar -czvf /tmp/fe.tar.gz -C /opt/starrocks/fe ."'
        )
    )

    copy_zip = BashOperator(
        task_id="copy_zip",
        bash_command=(
            'sshpass -p "password" scp -P 2222 root@starrocks-fe:/tmp/fe.tar.gz /tmp/fe.tar.gz'
        )
    )

    upload = PythonOperator(
        task_id="upload_to_s3",
        python_callable=upload_to_s3,
        op_kwargs={
            'filename': "/tmp/fe.tar.gz",
            'key': 'fe/fe.tar.gz',
            'bucket_name': 'starrockk'
        }
    )

    backup_and_copy >> copy_zip >> upload

```

#### Airflow Task to Restore MinIO from AWS S3
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime
import os

def download_from_s3(local_path, key, bucket_name):
    hook = S3Hook('aws_s3')
    hook.download_file(key=key, bucket_name=bucket_name, local_path=local_path, preserve_file_name=True, use_autogenerated_subdir=False)

with DAG(
    dag_id="restore_starrocks_fe",
    start_date=datetime(2025, 8, 11),
    schedule_interval=None,
    catchup=False,
) as dag:

    download = PythonOperator(
        task_id="download_from_s3",
        python_callable=download_from_s3,
        op_kwargs={
            'local_path': '/tmp',
            'key': 'fe/fe.tar.gz',
            'bucket_name': 'starrockk'
        }
    )

    copy_to_starrocks = BashOperator(
        task_id="copy_to_starrocks",
        bash_command=(
            'sshpass -p "password" scp -o StrictHostKeyChecking=no -P 2222 /tmp/fe.tar.gz root@starrocks-fe:/tmp/fe.tar.gz'
        )
    )

    extract_on_starrocks = BashOperator(
        task_id="extract_on_starrocks",
        bash_command=(
            'sshpass -p "password" ssh -o StrictHostKeyChecking=no -p 2222 root@starrocks-fe '
            '"tar -xzvf /tmp/fe.tar.gz -C /opt/starrocks/fe"'
        )
    )

  
    download >> copy_to_starrocks >> extract_on_starrocks

```